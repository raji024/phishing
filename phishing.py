# -*- coding: utf-8 -*-
"""phishing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KeC0gvlqVtf1dsw0btfqlnLQFkaQXLVi
"""

pip install --upgrade xgboost

!pip install whois

# üëá Make sure this path is correct
import pandas as pd
import requests
import whois
from datetime import datetime
from bs4 import BeautifulSoup

data = pd.read_csv(r'/content/drive/MyDrive/final.csv')
data.columns = data.columns.str.strip()

print("‚úÖ Loaded Columns:")
print(data.columns.tolist())

# Install if needed
!pip install pandas numpy scikit-learn xgboost imbalanced-learn python-whois beautifulsoup4 requests

# Import
import pandas as pd
import numpy as np
import requests
import whois
from urllib.parse import urlparse
from bs4 import BeautifulSoup

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import joblib

# 1Ô∏è‚É£ Install tqdm if needed
!pip install tqdm

# 2Ô∏è‚É£ Imports
import pandas as pd
from tqdm import tqdm
tqdm.pandas()
import requests
import whois
from datetime import datetime
from bs4 import BeautifulSoup

# 3Ô∏è‚É£ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 4Ô∏è‚É£ Load CSV from Drive
data = pd.read_csv(r
                   '/content/drive/MyDrive/final.csv')
data.columns = data.columns.str.strip()

print("‚úÖ Loaded Columns:")
print(data.columns.tolist())

# 5Ô∏è‚É£ Define the missing function
def get_html_keyword_count(url):
    try:
        response = requests.get(url, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')
        keywords = soup.find_all('meta', attrs={'name': 'keywords'})
        return len(keywords)
    except Exception as e:
        return 0

# 6Ô∏è‚É£ Apply with progress bar
print("\n‚úÖ Adding html_keyword_count (this may take time)...")
data['html_keyword_count'] = data['url'].progress_apply(get_html_keyword_count)
print("‚úÖ html_keyword_count added!")

print(data[['url', 'html_keyword_count']].head(10))

# ------------------------------------------
# ‚úÖ Define suspicious words for blacklist
# ------------------------------------------
BLACKLIST_KEYWORDS = [
    "login", "verify", "account", "secure",
    "update", "bank", "password"
]

# ------------------------------------------
# ‚úÖ Function to check if URL is "blacklisted"
# ------------------------------------------
def check_blacklist_flag(url):
    try:
        url = str(url).lower()
        for word in BLACKLIST_KEYWORDS:
            if word in url:
                return 1
        return 0
    except Exception:
        return 0

# ------------------------------------------
# ‚úÖ Add the blacklist_flag column with progress bar
# ------------------------------------------
from tqdm import tqdm
tqdm.pandas()

print("\n‚úÖ Adding blacklist_flag column (this is fast)...")
data['blacklist_flag'] = data['url'].progress_apply(check_blacklist_flag)
print("‚úÖ blacklist_flag column added!")

# ------------------------------------------
# ‚úÖ Preview to confirm
# ------------------------------------------
print(data[['url', 'blacklist_flag']].head(10))
data.to_csv('/content/dataset_phishing_augmented.csv', index=False)
print("‚úÖ CSV with new features saved!")

data = pd.read_csv(r'/content/drive/MyDrive/dataset_phishing_augmented (1).csv')
TARGET_COLUMN = 'status'  # Or your real target
X = data.drop(TARGET_COLUMN, axis=1)
y = data[TARGET_COLUMN]

!pip install pandas requests beautifulsoup4

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# üëá Make sure this path is correct
data = pd.read_csv(r'/content/drive/MyDrive/dataset_phishing_augmented (1).csv')
data.columns = data.columns.str.strip()

print("‚úÖ Loaded Columns:")
print(data.columns.tolist())

TARGET_COLUMN = 'status'   # ‚úÖ this is your actual label

# Drop label and the raw 'url' column from features
X = data.drop([TARGET_COLUMN, 'url'], axis=1)
y = data[TARGET_COLUMN]

print("\n‚úÖ X shape:", X.shape)
print("‚úÖ y distribution:\n", y.value_counts())

print("\n‚úÖ Checking for missing values:")
print(X.isnull().sum())

# Fill any NaNs with 0
X = X.fillna(0)
print("‚úÖ Missing values filled with 0.")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("\n‚úÖ Train/Test Split:")
print("Train samples:", X_train.shape[0])
print("Test samples :", X_test.shape[0])
X_train = X_train.values
X_test = X_test.values

"""check class Balance"""

print("\n‚úÖ Class balance before SMOTE:")
print(pd.Series(y_train).value_counts())

"""apply smote for balancing"""

smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

print("\n‚úÖ After SMOTE balancing:")
print(pd.Series(y_train_bal).value_counts())

param_dist = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2],
    'min_child_weight': [1, 3, 5]
}

"""setup xgboost classifier"""

xgb_model = XGBClassifier(
    objective='binary:logistic',
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42,
    verbosity=0
)

"""Setup RandomizedSearchCV"""

search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=30,
    scoring='accuracy',
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

"""n Hyperparameter Tuning"""

# ‚úÖ Convert label to numeric (safely)
data['status'] = data['status'].replace({
    'legitimate': 0,
    'phishing': 1
}).astype(int)

# ‚úÖ Define features and target
TARGET_COLUMN = 'status'
X = data.drop([TARGET_COLUMN, 'url'], axis=1)
y = data[TARGET_COLUMN]

print("\n‚úÖ X shape:", X.shape)
print("‚úÖ y distribution:\n", y.value_counts())

# ===========================================
# ‚úÖ 1Ô∏è‚É£ Import Libraries
# ===========================================
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

import xgboost as xgb

# ===========================================
# ‚úÖ 2Ô∏è‚É£ Load Data
# ===========================================
data = pd.read_csv('/content/dataset_phishing_augmented.csv')  # Change path if needed
print("\n‚úÖ Original Columns:")
print(data.columns.tolist())

# ===========================================
# ‚úÖ 3Ô∏è‚É£ Label Conversion
# ===========================================
data['status'] = data['status'].replace({
    'legitimate': 0,
    'phishing': 1
})
print("\n‚úÖ Label Conversion Done")
print(data['status'].value_counts())

# ===========================================
# ‚úÖ 4Ô∏è‚É£ Define Features and Target
# ===========================================
TARGET_COLUMN = 'status'
X = data.drop([TARGET_COLUMN, 'url'], axis=1)
y = data[TARGET_COLUMN]

print("\n‚úÖ X Shape:", X.shape)
print("‚úÖ y distribution:\n", y.value_counts())

# ===========================================
# ‚úÖ 5Ô∏è‚É£ Train-Test Split
# ===========================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("\n‚úÖ Train-Test Split Done")
print(f"Train Samples: {X_train.shape[0]}")
print(f"Test Samples : {X_test.shape[0]}")

# ===========================================
# ‚úÖ 6Ô∏è‚É£ Balance Training Data with SMOTE
# ===========================================
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
print("\n‚úÖ After SMOTE Balancing:")
print(y_train_bal.value_counts())

# ===========================================
# ‚úÖ 7Ô∏è‚É£ Define XGBoost Model
# ===========================================
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    verbosity=0,
    random_state=42
)

# ===========================================
# ‚úÖ 8Ô∏è‚É£ Define Parameter Grid
# ===========================================
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'n_estimators': [100, 150, 200],
    'subsample': [0.6, 0.7, 0.8],
    'colsample_bytree': [0.6, 0.7, 0.8],
    'reg_alpha': [0, 0.5, 1],
    'reg_lambda': [1, 1.5, 2]
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    scoring='accuracy',
    n_iter=20,
    cv=cv,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# ===========================================
# ‚úÖ 9Ô∏è‚É£ Hyperparameter Tuning (NO early stopping)
# ===========================================
print("\n‚úÖ Starting Hyperparameter Tuning (no early stopping in search)...")
search.fit(X_train_bal, y_train_bal)
print("\n‚úÖ Hyperparameter Tuning Complete!")

print("\n‚úÖ Best Hyperparameters Found:")
print(search.best_params_)

# ===========================================
# ‚úÖ üîü Refit Best Model with Early Stopping
# ===========================================
print("\n‚úÖ Refitting best model with early stopping...")
best_params = search.best_params_
final_model = xgb.XGBClassifier(
    **best_params,
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    verbosity=0,
    random_state=42
)

# ‚úÖ Older-version-compatible early stopping:
final_model.fit(
    X_train_bal, y_train_bal,
    eval_set=[(X_test, y_test)],
    early_stopping_rounds=20,
    verbose=False
)

# ===========================================
# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ Evaluate Model
# ===========================================
preds_train = final_model.predict(X_train)
preds_test = final_model.predict(X_test)

train_acc = accuracy_score(y_train, preds_train)
test_acc = accuracy_score(y_test, preds_test)

print("\n‚úÖ Final Accuracy Results:")
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Test Accuracy : {test_acc:.4f}")

print("\n‚úÖ Classification Report:")
print(classification_report(y_test, preds_test))

print("\n‚úÖ Confusion Matrix:")
print(confusion_matrix(y_test, preds_test))

import xgboost
print(xgboost.__version__)

# ===========================================
# ‚úÖ 1Ô∏è‚É£ Import Libraries
# ===========================================
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

import xgboost as xgb

# ===========================================
# ‚úÖ 2Ô∏è‚É£ Load Data
# ===========================================
data = pd.read_csv('/content/dataset_phishing_augmented.csv')  # Change path if needed
print("\n‚úÖ Original Columns:")
print(data.columns.tolist())

# ===========================================
# ‚úÖ 3Ô∏è‚É£ Label Conversion
# ===========================================
data['status'] = data['status'].replace({
    'legitimate': 0,
    'phishing': 1
})
print("\n‚úÖ Label Conversion Done")
print(data['status'].value_counts())

# ===========================================
# ‚úÖ 4Ô∏è‚É£ Define Features and Target
# ===========================================
TARGET_COLUMN = 'status'
X = data.drop([TARGET_COLUMN, 'url'], axis=1)
y = data[TARGET_COLUMN]

print("\n‚úÖ X Shape:", X.shape)
print("‚úÖ y distribution:\n", y.value_counts())

# ===========================================
# ‚úÖ 5Ô∏è‚É£ Train-Test Split
# ===========================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("\n‚úÖ Train-Test Split Done")
print(f"Train Samples: {X_train.shape[0]}")
print(f"Test Samples : {X_test.shape[0]}")

# ===========================================
# ‚úÖ 6Ô∏è‚É£ Balance Training Data with SMOTE
# ===========================================
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
print("\n‚úÖ After SMOTE Balancing:")
print(y_train_bal.value_counts())

# ===========================================
# ‚úÖ 7Ô∏è‚É£ Define XGBoost Model
# ===========================================
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    verbosity=0,
    random_state=42
)

# ===========================================
# ‚úÖ 8Ô∏è‚É£ Define Parameter Grid
# ===========================================
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'n_estimators': [100, 150, 200],
    'subsample': [0.6, 0.7, 0.8],
    'colsample_bytree': [0.6, 0.7, 0.8],
    'reg_alpha': [0, 0.5, 1],
    'reg_lambda': [1, 1.5, 2]
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    scoring='accuracy',
    n_iter=20,
    cv=cv,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# ===========================================
# ‚úÖ 9Ô∏è‚É£ Hyperparameter Tuning (NO early stopping)
# ===========================================
print("\n‚úÖ Starting Hyperparameter Tuning (no early stopping in search)...")
search.fit(X_train_bal, y_train_bal)
print("\n‚úÖ Hyperparameter Tuning Complete!")

print("\n‚úÖ Best Hyperparameters Found:")
print(search.best_params_)

# ===========================================
# ‚úÖ üîü Refit Best Model with Early Stopping (using callback)
# ===========================================
print("\n‚úÖ Refitting best model with early stopping using callback...")
best_params = search.best_params_

final_model = xgb.XGBClassifier(
    **best_params,
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    verbosity=0,
    random_state=42
)

# ‚úÖ Define EarlyStopping callback (safe in 3.0.2)
early_stopping_cb = xgb.callback.EarlyStopping(rounds=20)

final_model.fit(
    X_train_bal, y_train_bal,
    eval_set=[(X_test, y_test)],
    callbacks=[early_stopping_cb],
    verbose=False
)

# ===========================================
# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ Evaluate Model
# ===========================================
preds_train = final_model.predict(X_train)
preds_test = final_model.predict(X_test)

train_acc = accuracy_score(y_train, preds_train)
test_acc = accuracy_score(y_test, preds_test)

print("\n‚úÖ Final Accuracy Results:")
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Test Accuracy : {test_acc:.4f}")

print("\n‚úÖ Classification Report:")
print(classification_report(y_test, preds_test))

print("\n‚úÖ Confusion Matrix:")
print(confusion_matrix(y_test, preds_test))

# ===========================================
# ‚úÖ 1Ô∏è‚É£ Import Libraries
# ===========================================
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

import xgboost as xgb

# ===========================================
# ‚úÖ 2Ô∏è‚É£ Load Data
# ===========================================
data = pd.read_csv('/content/dataset_phishing_augmented.csv')  # Change path if needed
print("\n‚úÖ Original Columns:")
print(data.columns.tolist())

# ===========================================
# ‚úÖ 3Ô∏è‚É£ Label Conversion
# ===========================================
data['status'] = data['status'].replace({
    'legitimate': 0,
    'phishing': 1
})
print("\n‚úÖ Label Conversion Done")
print(data['status'].value_counts())

# ===========================================
# ‚úÖ 4Ô∏è‚É£ Define Features and Target
# ===========================================
TARGET_COLUMN = 'status'
X = data.drop([TARGET_COLUMN, 'url'], axis=1)
y = data[TARGET_COLUMN]

print("\n‚úÖ X Shape:", X.shape)
print("‚úÖ y distribution:\n", y.value_counts())

# ===========================================
# ‚úÖ 5Ô∏è‚É£ Train-Test Split
# ===========================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("\n‚úÖ Train-Test Split Done")
print(f"Train Samples: {X_train.shape[0]}")
print(f"Test Samples : {X_test.shape[0]}")

# ===========================================
# ‚úÖ 6Ô∏è‚É£ Balance Training Data with SMOTE
# ===========================================
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
print("\n‚úÖ After SMOTE Balancing:")
print(y_train_bal.value_counts())

# ===========================================
# ‚úÖ 7Ô∏è‚É£ Define XGBoost Model
# ===========================================
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    verbosity=0,
    random_state=42
)

# ===========================================
# ‚úÖ 8Ô∏è‚É£ Define Parameter Grid
# ===========================================
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'n_estimators': [100, 150, 200],
    'subsample': [0.6, 0.7, 0.8],
    'colsample_bytree': [0.6, 0.7, 0.8],
    'reg_alpha': [0, 0.5, 1],
    'reg_lambda': [1, 1.5, 2]
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    scoring='accuracy',
    n_iter=20,
    cv=cv,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# ===========================================
# ‚úÖ 9Ô∏è‚É£ Hyperparameter Tuning (NO early stopping)
# ===========================================
print("\n‚úÖ Starting Hyperparameter Tuning (no early stopping in search)...")
search.fit(X_train_bal, y_train_bal)
print("\n‚úÖ Hyperparameter Tuning Complete!")

print("\n‚úÖ Best Hyperparameters Found:")
print(search.best_params_)

# ===========================================
# ‚úÖ üîü Refit Best Model WITHOUT Early Stopping
# ===========================================
print("\n‚úÖ Refitting best model without early stopping...")
best_params = search.best_params_

final_model = xgb.XGBClassifier(
    **best_params,
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    verbosity=0,
    random_state=42
)

final_model.fit(X_train_bal, y_train_bal)

# ===========================================
# ‚úÖ 1Ô∏è‚É£1Ô∏è‚É£ Evaluate Model
# ===========================================
preds_train = final_model.predict(X_train)
preds_test = final_model.predict(X_test)

train_acc = accuracy_score(y_train, preds_train)
test_acc = accuracy_score(y_test, preds_test)

print("\n‚úÖ Final Accuracy Results:")
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Test Accuracy : {test_acc:.4f}")

print("\n‚úÖ Classification Report:")
print(classification_report(y_test, preds_test))

print("\n‚úÖ Confusion Matrix:")
print(confusion_matrix(y_test, preds_test))

import joblib

joblib.dump(final_model, 'xgb_phishing_model.pkl')